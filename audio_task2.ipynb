{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import os\n",
    "import librosa as lr\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Model\n",
    "from keras.layers import Input\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = \"/Users/harsh/Downloads/Audio_1/LibriSpeech/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=np.array((),dtype='int32')\n",
    "y=np.array((),dtype='int32')\n",
    "file = open(data_folder+'SPEAKERS.txt','r')\n",
    "for i in range(12):\n",
    "    file.readline()\n",
    "line = file.readline().split('|')\n",
    "Dict = {'F':0,'M':1}\n",
    "while (len(line)>1):\n",
    "    package = line[2].strip()\n",
    "    if(package=='dev-clean'):\n",
    "        y = np.append(y,Dict.get(line[1].strip()))\n",
    "        x = np.append(x,int(line[0].strip()))\n",
    "    line = file.readline().split('|')\n",
    "file.close() \n",
    "data = {}\n",
    "for i in range(len(x)):\n",
    "    data[int(x[i])] = np.array(())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
       "        1., 1., 1., 1., 0., 1., 0., 0., 1., 0., 0., 1., 1., 0., 1., 1., 0.,\n",
       "        0., 0., 0., 0., 1., 0.]),\n",
       " array([  84.,  174.,  251.,  422.,  652.,  777., 1272., 1462., 1673.,\n",
       "        1919., 1988., 1993., 2035., 2078., 2086., 2277., 2412., 2428.,\n",
       "        2803., 2902., 3000., 3081., 3170., 3536., 3576., 3752., 3853.,\n",
       "        5338., 5536., 5694., 5895., 6241., 6295., 6313., 6319., 6345.,\n",
       "        7850., 7976., 8297., 8842.]))"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#y,x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_y = dict(zip(x,y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dict_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_j = 0\n",
    "j = 0\n",
    "for subdirs,dirs,files in os.walk(data_folder+'dev-clean'):\n",
    "    t_j+=1\n",
    "for subdirs,dirs,files in os.walk(data_folder+'dev-clean'):\n",
    "    for file in files:\n",
    "        if file.endswith('.flac'):\n",
    "            audio,sf = lr.load(subdirs+'/'+file)\n",
    "            data[int(subdirs.split('/')[-2])] = np.append(data[int(subdirs.split('/')[-2])],audio)    \n",
    "    j+=1\n",
    "#     print(float(j)/t_j*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.plot(np.arange(0,len(data[x[1]]))/sf,data[x[1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mel Filterbanks (not MFCC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17.5\n"
     ]
    }
   ],
   "source": [
    "seg = 550      #0.025*sf\n",
    "log_en = np.zeros((100,26))\n",
    "for i in range(len(x)):\n",
    "    l = 0\n",
    "    ctr = 0\n",
    "    while(l<len(data.get(x[i]))):\n",
    "        pow_spec = (np.abs(np.fft.fftshift(np.fft.fft(data.get(x[i])[l:l+seg],512))[:257])**2)/seg\n",
    "        mfb = lr.filters.mel(sr=22050,n_fft=512,n_mels=26)\n",
    "        energy = np.matmul(mfb,pow_spec)\n",
    "        energy = np.where(energy == 0, np.finfo(float).eps, energy)\n",
    "        log_en[ctr%100] = 20*np.log10(energy)\n",
    "        if(ctr%100==99):\n",
    "            fig = plt.figure(ctr,frameon=False)\n",
    "            ax = plt.Axes(fig, [0., 0., 1., 1.])\n",
    "            ax.set_axis_off()\n",
    "            fig.add_axes(ax)\n",
    "            ax.imshow(log_en,aspect='auto')\n",
    "            fig.savefig(data_folder+'dev-clean/Spectrograms/'+str(int(x[i]))+'_png/'+str(ctr//100)+'.png')\n",
    "            plt.close()\n",
    "        ctr += 1\n",
    "        l += 220\n",
    "        #print(float(l+1)/len(data.get(x[i]))*100)\n",
    "    print(float(i+1)/len(x)*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 17493 images belonging to 40 classes.\n",
      "Found 1928 images belonging to 40 classes.\n"
     ]
    }
   ],
   "source": [
    "im_size = 256\n",
    "train_gen = ImageDataGenerator(rescale=1./255,validation_split=0.1)\n",
    "train_set = train_gen.flow_from_directory(data_folder+'dev-clean/Spectrograms/',subset='training',target_size=(im_size,im_size),color_mode='rgb',class_mode='categorical',batch_size=50)\n",
    "test_set = train_gen.flow_from_directory(data_folder+'dev-clean/Spectrograms/',subset='validation',target_size=(im_size,im_size),color_mode='rgb',class_mode='categorical',batch_size=50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "im_size = 256\n",
    "inpt = Input((im_size,im_size,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "c1 = Conv2D(8,(3,3),activation='relu',padding='same')(inpt)\n",
    "#c12 = Conv2D(8,(3,3),activation='relu',padding='same')(c1)\n",
    "m1 = MaxPooling2D((2,2))(c1)\n",
    "c2 = Conv2D(16,(3,3),activation='relu',padding='same')(m1)\n",
    "#c22 = Conv2D(16,(3,3),activation='relu',padding='same')(c2)\n",
    "m2 = MaxPooling2D((2,2))(c2)\n",
    "#c3 = Conv2D(32,(3,3),activation='relu',padding='same')(m2)\n",
    "#c32 = Conv2D(32,(3,3),activation='relu',padding='same')(c3)\n",
    "#m3 = MaxPooling2D((2,2))(c3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = Flatten()(m2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifier Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#d1 = Dense(64,activation='relu')(f)\n",
    "#drp1 = Dropout(0.2)(d1)\n",
    "#d2 = Dense(64,activation='relu')(d1)\n",
    "#drp2 = Dropout(0.5)(d2)\n",
    "outpt = Dense(40,activation='softmax')(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(inpt,outpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_6 (InputLayer)         (None, 256, 256, 3)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_11 (Conv2D)           (None, 256, 256, 8)       224       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_11 (MaxPooling (None, 128, 128, 8)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_12 (Conv2D)           (None, 128, 128, 16)      1168      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_12 (MaxPooling (None, 64, 64, 16)        0         \n",
      "_________________________________________________________________\n",
      "flatten_6 (Flatten)          (None, 65536)             0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 40)                2621480   \n",
      "=================================================================\n",
      "Total params: 2,622,872\n",
      "Trainable params: 2,622,872\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='rmsprop',loss='categorical_crossentropy',metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "220/220 [==============================] - 344s 2s/step - loss: 3.4582 - acc: 0.1403 - val_loss: 2.9661 - val_acc: 0.2000\n",
      "Epoch 2/10\n",
      "220/220 [==============================] - 306s 1s/step - loss: 2.6626 - acc: 0.2714 - val_loss: 2.6078 - val_acc: 0.2987\n",
      "Epoch 3/10\n",
      "220/220 [==============================] - 278s 1s/step - loss: 2.2486 - acc: 0.3803 - val_loss: 2.2295 - val_acc: 0.3755\n",
      "Epoch 4/10\n",
      "220/220 [==============================] - 275s 1s/step - loss: 1.8100 - acc: 0.4943 - val_loss: 2.0556 - val_acc: 0.4193\n",
      "Epoch 5/10\n",
      "220/220 [==============================] - 277s 1s/step - loss: 1.5486 - acc: 0.5595 - val_loss: 1.8595 - val_acc: 0.4718\n",
      "Epoch 6/10\n",
      "220/220 [==============================] - 289s 1s/step - loss: 1.2897 - acc: 0.6248 - val_loss: 1.7136 - val_acc: 0.5232\n",
      "Epoch 7/10\n",
      "220/220 [==============================] - 325s 1s/step - loss: 1.0920 - acc: 0.6834 - val_loss: 1.6095 - val_acc: 0.5418\n",
      "Epoch 8/10\n",
      "220/220 [==============================] - 303s 1s/step - loss: 1.0478 - acc: 0.6949 - val_loss: 1.6474 - val_acc: 0.5260\n",
      "Epoch 9/10\n",
      "220/220 [==============================] - 303s 1s/step - loss: 0.8713 - acc: 0.7425 - val_loss: 1.5641 - val_acc: 0.5622\n",
      "Epoch 10/10\n",
      "220/220 [==============================] - 287s 1s/step - loss: 0.8088 - acc: 0.7657 - val_loss: 1.6873 - val_acc: 0.5600\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x683a8eda0>"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(train_set,steps_per_epoch=220,epochs=10,validation_data=test_set,validation_steps=22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights('audio_cnn2.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_audio,sf = lr.load('/Users/harsh/Downloads/Audio_1/sample_debate.mp3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seg = 550      #0.025*sf\n",
    "log_en = np.zeros((100,26))\n",
    "l = 0\n",
    "ctr = 0\n",
    "while(l<len(test_audio)):\n",
    "    pow_spec = (np.abs(np.fft.fftshift(np.fft.fft(test_audio[l:l+seg],512))[:257])**2)/seg\n",
    "    mfb = lr.filters.mel(sr=22050,n_fft=512,n_mels=26)\n",
    "    energy = np.matmul(mfb,pow_spec)\n",
    "    energy = np.where(energy == 0, np.finfo(float).eps, energy)\n",
    "    log_en[ctr%100] = 20*np.log10(energy)\n",
    "    if(ctr%100==99):\n",
    "        fig = plt.figure(ctr,frameon=False)\n",
    "        ax = plt.Axes(fig, [0., 0., 1., 1.])\n",
    "        ax.set_axis_off()\n",
    "        fig.add_axes(ax)\n",
    "        ax.imshow(log_en,aspect='auto')\n",
    "        fig.savefig('/Users/harsh/Downloads/Audio_1/sample_debate_png/'+str(ctr//100)+'.png')\n",
    "        plt.close()\n",
    "    #print(float(l+1)/len(test_audio)*100)\n",
    "    ctr += 1\n",
    "    l += 220"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set.class_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pred_map = {0:'Female',1:'Male'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "174311"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#ctr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "ftxt = open('/Users/harsh/Downloads/Audio_1/sample_debate_png/prediction.txt','w+')\n",
    "for i in range(ctr//100):\n",
    "    pred_img = cv2.imread('/Users/harsh/Downloads/Audio_1/sample_debate_png/'+str(i)+'.png')\n",
    "    pred_img = cv2.resize(pred_img,(im_size,im_size))\n",
    "    pred_img = pred_img/255\n",
    "    pred_img = np.expand_dims(pred_img,axis=0)\n",
    "    pred = model.predict(pred_img)\n",
    "    #pred = 1 if(pred>0.5) else 0\n",
    "    ftxt.write(str(i)+' : '+str(np.argmax(pred))+'\\n')  \n",
    "#pred\n",
    "ftxt.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('audio_cnn2.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = Model(inpt,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctr = 174300\n",
    "features = []\n",
    "for i in range(ctr//100):\n",
    "    pred_img = cv2.imread('/Users/harsh/Downloads/Audio_1/sample_debate_png/'+str(i)+'.png')\n",
    "    pred_img = cv2.resize(pred_img,(im_size,im_size))\n",
    "    pred_img = pred_img/255\n",
    "    pred_img = np.expand_dims(pred_img,axis=0)\n",
    "    #pred = model.predict(pred_img)\n",
    "    features.append(model2.predict(pred_img))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = np.array(features).reshape(len(features),-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1743, 65536)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Elbow Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1\n",
      "0.2\n",
      "0.3\n",
      "0.4\n",
      "0.5\n",
      "0.6\n",
      "0.7\n",
      "0.8\n",
      "0.9\n",
      "1.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x65bd91cc0>]"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wcss = []\n",
    "for i in tqdm(range(1,11)):\n",
    "    kmeans = KMeans(i,init='k-means++')\n",
    "    kmeans.fit(features)\n",
    "    wcss.append(kmeans.inertia_)\n",
    "plt.plot(range(1,11),wcss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(6,init='k-means++')\n",
    "y_kmeans = kmeans.fit_predict(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_time = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique,counts = np.unique(y_kmeans,return_counts=True)\n",
    "dict_time = dict(zip(unique,counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 291, 1: 296, 2: 371, 3: 15, 4: 427, 5: 343}"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_j = 0\n",
    "j = 0\n",
    "x_svm = []\n",
    "y_svm = []\n",
    "for subdirs,dirs,files in os.walk(data_folder+'dev-clean/Spectrograms'):\n",
    "    t_j+=1\n",
    "for subdirs,dirs,files in os.walk(data_folder+'dev-clean/Spectrograms'):\n",
    "    for file in files:\n",
    "        if file.endswith('.png'):\n",
    "            image = cv2.imread(subdirs+'/'+file)\n",
    "            image = cv2.resize(image,(im_size,im_size))\n",
    "            image = np.expand_dims(image,axis=0)\n",
    "            x_svm.append(model2.predict(image))\n",
    "            y_svm.append(dict_y.get(int(subdirs.split('/')[-1][:-4])))\n",
    "    j+=1\n",
    "#     print(float(j)/t_j*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_svm2 = x_svm\n",
    "y_svm2 = y_svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_svm = []\n",
    "y_svm = []\n",
    "test_indices = random.sample(range(len(x_svm2)),1000)\n",
    "for i in test_indices:\n",
    "    x_svm.append(x_svm2[i])\n",
    "    y_svm.append(y_svm2[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_svm = np.array(x_svm).reshape(len(x_svm),-1)\n",
    "y_svm = np.array(y_svm).reshape(len(y_svm),-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 65536)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_svm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_svm_train,x_svm_test,y_svm_train,y_svm_test = train_test_split(x_svm,y_svm,test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc =  StandardScaler()\n",
    "x_svm_train = sc.fit_transform(x_svm_train)\n",
    "x_svm_test = sc.transform(x_svm_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm = SVC(kernel='rbf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/utils/validation.py:761: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/usr/local/lib/python3.7/site-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='auto_deprecated',\n",
       "  kernel='rbf', max_iter=-1, probability=False, random_state=None,\n",
       "  shrinking=True, tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm.fit(x_svm_train,y_svm_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_svm_pred = svm.predict(x_svm_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_svm_test,y_svm_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[67, 35],\n",
       "       [34, 64]])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_svm_pred = svm.predict(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1743,)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_svm_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 ... 0 0 0] [3 3 3 ... 3 3 3]\n"
     ]
    }
   ],
   "source": [
    "print(y_svm_pred,y_kmeans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_gender = {}\n",
    "for i in range(6):\n",
    "    for j in range(len(y_kmeans)):\n",
    "        c1 = 0\n",
    "        c2 = 0\n",
    "        if(y_kmeans[j]==i):\n",
    "            c1 += 1\n",
    "            c2 += y_svm_pred[i]\n",
    "        if(c1!=0):\n",
    "            dict_gender[i] = 1 if(float(c2)/c1>0.5) else 0\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 291, 1: 296, 2: 371, 3: 15, 4: 427, 5: 343}"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
